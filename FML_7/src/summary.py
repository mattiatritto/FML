# -*- coding: utf-8 -*-
"""Summary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kYvRRvgoqTRCmdMfitEq4ipt7nB6tzX
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler
import torch
from torch import nn
from torch import optim

# Variables to modify
dataset_path = "sample_data/diabetes.csv"
target_feature = "Outcome"
random_state = 42
torch.manual_seed(random_state)
hidden_size = 50
output_size = 1
epochs = 3000





# Preprocessing steps
dataset = pd.read_csv(dataset_path)
X = dataset.drop([target_feature], axis=1).values
y = dataset[target_feature].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)
scaler = RobustScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)





# Neural Network definition
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)



class SimpleNN_1(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    """
    Constructor method that initialize our Neural Network.
    :param input_size: size of our data
    :param hidden_size: neurons in the middle layer
    :output_size: neurons in the output layer (1 in a binary classification)
    """
    super(SimpleNN_1, self).__init__()
    self.layer_1 = nn.Linear(input_size, hidden_size)
    self.layer_2 = nn.Linear(hidden_size, output_size)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    x = self.layer_1(x)
    x = self.sigmoid(x)
    x = self.layer_2(x)
    x = self.sigmoid(x)
    return x



def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item()
  acc = (correct / len(y_pred)) * 100
  return acc





# Neural Network creation

input_size = X.shape[1]
model = SimpleNN_1(input_size, hidden_size, output_size).to(device)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001) # Tells to the model how to update its internal parameters
X_train_std = torch.from_numpy(X_train_std).float().to(device)
X_test_std = torch.from_numpy(X_test_std).float().to(device)
y_train = torch.from_numpy(y_train).float().to(device)
y_test = torch.from_numpy(y_test).float().to(device)

for epoch in range(epochs):
  model.train()
  outputs = model(X_train_std)
  outputs = outputs.squeeze()
  loss = criterion(outputs, y_train)
  outputs = torch.round(outputs).float()
  acc = accuracy_fn(y_true=y_train, y_pred=outputs)
  loss.backward() # Computes the loss gradients wrt the model's parameters
  optimizer.step() # Update the model parameters based on the computed gradients
  optimizer.zero_grad() # For each epoch it is best practice to set to 0 the previous gradients
  model.eval()

  with torch.inference_mode(): # Now we don't need to compute gradients
    test_outputs = model(X_test_std)
    test_outputs = test_outputs.squeeze()
    test_loss = criterion(test_outputs, y_test)
    test_outputs = torch.round(test_outputs).float()
    test_acc = accuracy_fn(y_true=y_test, y_pred=test_outputs)

  if (epoch + 1) % 20 == 0:
      print(f"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")